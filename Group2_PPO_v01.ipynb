{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ea6e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install gymnasium\n",
    "#!pip install stable-baselines3[gymnasium]\n",
    "#!pip install swig\n",
    "#!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a1e394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shrey\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:52: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shrey\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:52: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arg specs do not match: original=FullArgSpec(args=['input', 'dtype', 'name', 'layout'], varargs=None, varkw=None, defaults=(None, None, None), kwonlyargs=[], kwonlydefaults=None, annotations={}), new=FullArgSpec(args=['input', 'dtype', 'name'], varargs=None, varkw=None, defaults=(None, None), kwonlyargs=[], kwonlydefaults=None, annotations={}), fn=<function ones_like_v2 at 0x0000015685C60C10>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecordVideo, RecordEpisodeStatistics\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\__init__.py:20\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Tools for probabilistic reasoning in TensorFlow.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Contributors to the `python/` dir should not alter this file; instead update\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# `python/__init__.py` as necessary.\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m substrates\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\substrates\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m all_util\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lazy_loader  \u001b[38;5;66;03m# pylint: disable=g-direct-tensorflow-import\u001b[39;00m\n\u001b[0;32m     25\u001b[0m jax \u001b[38;5;241m=\u001b[39m lazy_loader\u001b[38;5;241m.\u001b[39mLazyLoader(\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mglobals\u001b[39m(),\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow_probability.substrates.jax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:142\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _tf_loaded():\n\u001b[0;32m    140\u001b[0m   \u001b[38;5;66;03m# Non-lazy load of packages that register with tensorflow or keras.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m pkg_name \u001b[38;5;129;01min\u001b[39;00m _maybe_nonlazy_load:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forces loading the package from its lazy loader.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m all_util\u001b[38;5;241m.\u001b[39mremove_undocumented(\u001b[38;5;18m__name__\u001b[39m, _lazy_load \u001b[38;5;241m+\u001b[39m _maybe_nonlazy_load)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py:61\u001b[0m, in \u001b[0;36mLazyLoader.__dir__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dir__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 61\u001b[0m   module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(module)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py:44\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_first_access \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_module_globals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_module_globals[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_name] \u001b[38;5;241m=\u001b[39m module\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\experimental\\__init__.py:35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_batching\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bijectors\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\experimental\\bijectors\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Probability Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"TensorFlow Probability experimental bijectors package.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbijectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mldj_ratio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_log_det_jacobian_ratio\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbijectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mldj_ratio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inverse_log_det_jacobian_ratio\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbijectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribution_bijectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_distribution_bijector\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\__init__.py:23\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import,wildcard-import,line-too-long,g-importing-member\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbijectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabsolute_value\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AbsoluteValue\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbijectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mascending\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ascending\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbijectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_normalization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchNormalization\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\absolute_value.py:23\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbijectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bijector\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_util\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtype_util\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_util\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_composite_tensor\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m batch_shape_lib\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache_util\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtype_util\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\batch_shape_lib.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_static \u001b[38;5;28;01mas\u001b[39;00m ps\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_util\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorshape_util\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\prefer_static.py:325\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mones(s_, dtype_util\u001b[38;5;241m.\u001b[39mas_numpy_dtype(dtype \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m    324\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mones(s, dtype \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, name)\n\u001b[1;32m--> 325\u001b[0m ones_like \u001b[38;5;241m=\u001b[39m \u001b[43m_copy_docstring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_ones_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rank\u001b[39m(\u001b[38;5;28minput\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-builtin,unused-argument\u001b[39;00m\n\u001b[0;32m    329\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\prefer_static.py:86\u001b[0m, in \u001b[0;36m_copy_docstring\u001b[1;34m(original_fn, new_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m new_spec \u001b[38;5;241m=\u001b[39m tf_inspect\u001b[38;5;241m.\u001b[39mgetfullargspec(new_fn)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_spec \u001b[38;5;241m!=\u001b[39m new_spec:\n\u001b[1;32m---> 86\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     87\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArg specs do not match: original=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, new=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, fn=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     88\u001b[0m           original_spec, new_spec, original_fn))\n\u001b[0;32m     89\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\u001b[38;5;241m.\u001b[39mdecorator\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(wrapped_fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     91\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m wrapped_fn\n",
      "\u001b[1;31mValueError\u001b[0m: Arg specs do not match: original=FullArgSpec(args=['input', 'dtype', 'name', 'layout'], varargs=None, varkw=None, defaults=(None, None, None), kwonlyargs=[], kwonlydefaults=None, annotations={}), new=FullArgSpec(args=['input', 'dtype', 'name'], varargs=None, varkw=None, defaults=(None, None), kwonlyargs=[], kwonlydefaults=None, annotations={}), fn=<function ones_like_v2 at 0x0000015685C60C10>"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, initializers\n",
    "from tensorflow.keras.initializers import Orthogonal, Constant\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo, RecordEpisodeStatistics\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2046cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(Model):\n",
    "\n",
    "    \"\"\" PPO model for continuous action space. See Sutton & Barto 13.7 Continuous Action Spaces \"\"\"\n",
    "\n",
    "    def __init__(self, env,ppo_clip_val=0.2, learning_rate=1e-3, epsilon=1e-5):\n",
    "        super(PPO, self).__init__()\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.observation_shape = self.env.observation_space.shape\n",
    "        self.action_shape = self.env.action_space.shape\n",
    "\n",
    "        self.__create_shared_layers()\n",
    "        self.__create_actor()\n",
    "        self.__create_critic()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.optimizer = Adam(learning_rate=self.learning_rate, epsilon=self.epsilon)\n",
    "\n",
    "        # PPO PARAMS\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "\n",
    "    def __layer_init(self, units, std=np.sqrt(2), bias_const=0.0, activation=None):\n",
    "        \"\"\" Initialize layers with orthogonal weights and constant biases \"\"\"\n",
    "        return layers.Dense(units, kernel_initializer=Orthogonal(gain=std),\n",
    "                            bias_initializer=Constant(value=bias_const), activation=activation)\n",
    "\n",
    "    def __create_shared_layers(self):\n",
    "        \"\"\" Create shared layers for both actor and critic networks \"\"\"\n",
    "        self.shared_input = layers.Input(shape=self.observation_shape)\n",
    "        x = self.__layer_init(64, activation='tanh')(self.shared_input)\n",
    "        x = self.__layer_init(64, activation='tanh')(x)\n",
    "        self.shared_output = x\n",
    "\n",
    "    def __create_actor(self):\n",
    "        \"\"\" Create continuous actor neural network \"\"\"\n",
    "        mu_output = self.__layer_init(self.action_shape[0], std=0.01, activation='tanh')(self.shared_output)\n",
    "        log_std_output = layers.Dense(self.action_shape[0], activation='softplus')(self.shared_output)  # todo - IS THIS CORRECT?\n",
    "        self.actor = Model(inputs=self.shared_input, outputs=[mu_output, log_std_output], name='actor')\n",
    "\n",
    "    def __create_critic(self):\n",
    "        \"\"\" Create critic neural network \"\"\"\n",
    "        critic_output = self.__layer_init(1, std=1.0)(self.shared_output)\n",
    "        self.critic = Model(inputs=self.shared_input, outputs=critic_output, name='critic')\n",
    "\n",
    "    def get_value(self, observation):\n",
    "        \"\"\" Get value vector \"\"\"\n",
    "        return self.critic(observation)\n",
    "\n",
    "    def get_policy(self, observation):\n",
    "        \"\"\" Get action vector and log std \"\"\"\n",
    "        return self.actor(observation)\n",
    "\n",
    "    def sample_action(self, observation):\n",
    "\n",
    "        \"\"\" Sample action from the policy distribution \"\"\"\n",
    "        mu, log_std = self.get_policy(observation)\n",
    "        std = tf.exp(log_std)\n",
    "        action = tf.random.normal(shape=mu.shape, mean=mu, stddev=std)  # todo - IS THIS CORRECT?\n",
    "        return action\n",
    "    \n",
    "    def clip(self, inputs):\n",
    "        \n",
    "        return tf.clip_by_value(inputs, 1-self.ppo_clip_val, 1+self.ppo_clip_val)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ## ============ ** check from here ** ============\n",
    "    \n",
    "    # PSEUDOCODE ONLY - MISSING VARIABLES \n",
    "    def L_clip(self):\n",
    "\n",
    "        policy_ratio = self.policy_ratio(observation, old_prob)\n",
    "\n",
    "        clipped_ratio = self.clip(policy_ratio)\n",
    "\n",
    "        clipped_loss = clipped_ratio * gaes\n",
    "\n",
    "        full_loss = policy_ratio * gaes\n",
    "\n",
    "        policy_loss = min(full_loss, clipped_loss).mean()  # todo - if minimising, make this negative\n",
    "\n",
    "\n",
    "    def train_policy_network(self, observations, actions, old_action_probs, advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, log_std = self.actor_critic.actor(observations)\n",
    "            std = tf.exp(log_std)\n",
    "            dist = tfp.distributions.Normal(mu, std)\n",
    "            new_action_probs = dist.prob(actions)\n",
    "\n",
    "            ratios = new_action_probs / old_action_probs\n",
    "            clipped_ratios = tf.clip_by_value(ratios, 1.0 - self.ppo_clip_val, 1.0 + self.ppo_clip_val)\n",
    "            loss = -tf.reduce_mean(tf.minimum(ratios * advantages, clipped_ratios * advantages))\n",
    "\n",
    "        grads = tape.gradient(loss, self.actor_critic.actor.trainable_variables)\n",
    "        self.policy_optimizer.apply_gradients(zip(grads, self.actor_critic.actor.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def train_value_network(self, observations, returns):\n",
    "        with tf.GradientTape() as tape:\n",
    "            values = tf.squeeze(self.actor_critic.critic(observations))\n",
    "            loss = tf.reduce_mean(tf.square(returns - values))\n",
    "\n",
    "        grads = tape.gradient(loss, self.actor_critic.critic.trainable_variables)\n",
    "        self.value_optimizer.apply_gradients(zip(grads, self.actor_critic.critic.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones, next_value):\n",
    "        values = np.append(values, next_value)\n",
    "        gaes = np.zeros_like(rewards)\n",
    "        last_gae_lam = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            gaes[t] = last_gae_lam = delta + self.gamma * self.alpha * (1 - dones[t]) * last_gae_lam\n",
    "        return gaes\n",
    "\n",
    "    def train(self, env, total_timesteps, batch_size=64):\n",
    "\n",
    "        # Lists to store the trajectory data for a single batch\n",
    "        observations = []  # To store observations (states) encountered during the episode\n",
    "        actions = []  # To store actions taken by the agent\n",
    "        rewards = []  # To store rewards received after taking actions\n",
    "        dones = []  # To store done flags indicating whether the episode has ended\n",
    "        values = []  # To store value estimates from the critic network\n",
    "        action_probs = []  # To store the probabilities of the actions taken by the policy network\n",
    "\n",
    "        observation, _ = env.reset()\n",
    "        for timestep in range(total_timesteps):\n",
    "            action, action_prob = self.select_action(observation)\n",
    "            next_observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            value = self.actor_critic.get_value(observation)\n",
    "\n",
    "            observations.append(observation)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            values.append(value)\n",
    "            action_probs.append(action_prob)\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "            if done:\n",
    "                observation, _ = env.reset()\n",
    "\n",
    "            if (timestep + 1) % batch_size == 0:\n",
    "                next_value = self.actor_critic.get_value(observation)\n",
    "                returns = self.compute_gae(rewards, values, dones, next_value)\n",
    "\n",
    "                observations = np.array(observations)\n",
    "                actions = np.array(actions)\n",
    "                action_probs = np.array(action_probs)\n",
    "                returns = np.array(returns)\n",
    "                advantages = returns - np.array(values)\n",
    "\n",
    "                for _ in range(self.max_policy_updates):\n",
    "                    policy_loss = self.train_policy_network(observations, actions, action_probs, advantages)\n",
    "                    if policy_loss < self.target_kl:\n",
    "                        break\n",
    "\n",
    "                for _ in range(self.max_value_updates):\n",
    "                    value_loss = self.train_value_network(observations, returns)\n",
    "\n",
    "                observations = []\n",
    "                actions = []\n",
    "                rewards = []\n",
    "                dones = []\n",
    "                values = []\n",
    "                action_probs = []\n",
    "    \n",
    "    # todo - not working on jupyter notebook\n",
    "    def play(self, render=False, record=False, video_folder='videos', episode_trigger=lambda e: True, name_prefix=\"rl-video\"):\n",
    "\n",
    "        env = RecordEpisodeStatistics(self.env)\n",
    "\n",
    "        if record:\n",
    "            env = RecordVideo(env, video_folder)\n",
    "            env.metadata['render_fps'] = 30  # Set the FPS to 30\n",
    "            \n",
    "        observation, _ = env.reset()\n",
    "        observation = np.array([observation])\n",
    "        print(observation.shape)\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            action = self.sample_action(observation)[0]\n",
    "            observation, reward, done, _, _ = env.step(action)\n",
    "            observation = np.array([observation])\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "        env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333754c",
   "metadata": {},
   "source": [
    "###### https://www.gymlibrary.dev/environments/box2d/bipedal_walker/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a98b63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BipedalWalker-v3', render_mode='rgb_array')\n",
    "\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# e.g. test multiple observations being fed to the neural net for when batch training\n",
    "obs = np.array([obs])\n",
    "\n",
    "a1 = PPO(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ae6e25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.00184107,  0.00191982,  0.00068387, -0.00229931]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.3325026 , 0.56307745, 0.5883078 , 0.65620816]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# get the policy output (mu vector, log_std vector)\n",
    "print(a1.get_policy(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e36847a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 2.2131956  -0.67672855 -0.10302535  5.2636023 ]], shape=(1, 4), dtype=float32) [[-1.2076478   0.6972355   0.2103979   0.65136325]]\n"
     ]
    }
   ],
   "source": [
    "# sample an action\n",
    "print(a1.sample_action(obs), np.array(a1.sample_action(obs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a418bc61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
